// source this function into an R session using the Rcpp::sourceCpp
// function (or via the Source button on the editor toolbar). Learn
// more about Rcpp at:
//
//   http://www.rcpp.org/
//   http://adv-r.had.co.nz/Rcpp.html
//   http://gallery.rcpp.org/
//
// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) {
return x * 2;
}
using namespace Rcpp;
#include <Rcpp.h>
using namespace Rcpp;
timesTwo(42)
/*** R
timesTwo(42)
*/
?rnorm
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
T=200; df=2
x=vector("numeric")
y=vector("numeric")
x[1]=rt(1,df=df)
for(n in 1:(T+1)){
x[n+1]=rho*x[n]+tau*rt(1,df=df)
y[n]=x[n]+sigma*rt(1,df=df)
}
rho=0.8; tau=1; sigma=0.5; T=200; df=2 #0<df<1 possible
### Calculate y_[0:T] and trajectory x*_[0:T]
x=vector("numeric")
y=vector("numeric")
x[1]=rt(1,df=df)
for(n in 1:(T+1)){
x[n+1]=rho*x[n]+tau*rt(1,df=df)
y[n]=x[n]+sigma*rt(1,df=df)
}
N=10000; sigma_rho=0.1; sigma_tau=0.1; sigma_sigma=0.1 #N=
rho_0=0.8; tau_0=1.0; sigma_0=0.5
N=1000
nsteps=100
test=gmhpfull(rho_0,tau_0,sigma_0,sigma_rho,sigma_tau,sigma_sigma,nsteps,y,df,N,T);
test.rho=test[[1]]
test.tau=test[[2]]
test.sigma=test[[3]]
loglik=log(test[[4]]
)
hist(test.rho)
hist(test.tau)
hist(test.sigma)
par(mfrow=c(2,2))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
par(mfrow=c(2,2))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
rho_0=0.8; tau_0=1.0; sigma_0=0.5; nsteps=100; N=1000
test=gmhpfull(rho_0,tau_0,sigma_0,sigma_rho,sigma_tau,sigma_sigma,nsteps,y,df,N,T);
#remove burn-in
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho)) #0.17
1-rejectionRate(mcmc(test.tau)) #0.27
1-rejectionRate(mcmc(test.sigma)) #0.26
par(mfrow=c(2,2))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
test=gmhpfull(rho_0,tau_0,sigma_0,sigma_rho,sigma_tau,sigma_sigma,nsteps,y,df,N,T);
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
Rcpp::sourceCpp('Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R/p.cpp')
test=gmhpfull(rho_0,tau_0,sigma_0,sigma_rho,sigma_tau,sigma_sigma,nsteps,y,df,N,T);
library(coda)
library(doParallel)
install.packages("doParallel")
install.packages("doParallel")
library(doParallel)
library(doParallel)
spreadin3
rm(list=ls())
rm(list=ls())
#install.packages("R.matlab")
#install.packages("zoo")
#install.packages("tseries")
#install.packages("fUnitRoots")
#install.packages("urca")
#install.packages("vars")
library(tseries) #adf.test; kpss.test
library(fUnitRoots) #adfTest
library(urca) #ca.jo
library(vars)
library(R.matlab)
library(zoo)
setwd("/Users/tillkischkat/Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/Cointegration")
source("Cointegration_functions.R")
setwd("/Users/tillkischkat/Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/Spread")
source("Spread_functions.R")
spx <- readMat("spx.mat")
spximport <- spx$pp
#aquire data
tickers=spximport[1,1,1]$tickers
names=spximport[2,1,1]$names
sector=spximport[3,1,1]$sector
dt=Matlab2Rdate(spximport[4,1,1]$dt)
Index=spximport[5,1,1]$Index
w=spximport[6,1,1]$w
px=spximport[7,1,1]$px
px.clean=spximport[8,1,1]$px.clean
#calculate daily log-returns
ret=log(px.clean[2:dim(px.clean)[1],])-log(px.clean[1:(dim(px.clean)[1]-1),])
ret[is.na(ret)]=0
ret[is.infinite(ret)]=0
#days in each month
days=diff(seq(as.Date(dt[1]), as.Date(dt[length(dt)]), by = "month"))
days=c(days,19)
w=rbind(w,rep(0,ncol(w)))
w.days=w[rep(1:nrow(w), times = days), ]
#stock index daily returns
ret=rbind(rep(0,ncol(ret)),ret)
index.ret=vector("numeric"); cumul.index.ret=vector("numeric")
cumul.index.ret[1]=100
for(i in 1:nrow(ret)){
index.ret[i]=sum(ret[i,]*w.days[i,])
cumul.index.ret[i+1]=cumul.index.ret[i]*(1+index.ret[i])
}
#100 stock in chosen index
index100=index.retN(100,w.days,ret)
ret100=index100[[1]]
cumul100=index100[[2]]
#diagnostic check
sd(ret100-index.ret)
par(mfrow=c(2,1))
ts.plot(index.ret,main="S&P 500 1.1.1990-19.3.2014",ylab="Daily log-returns of index")
lines(ret100,col="red",lty=3)
legend("bottomright",c("All stocks","Largest 100 stocks"),lty=c(1,3),col=c("black","red"),cex=0.5)
ts.plot(cumul.index.ret,main="S&P 500 1.1.1990-19.3.2014",ylab="Daily cumulative log-returns of index")
lines(cumul100,col="red",lty=3)
legend("bottomright",c("All stocks","Largest 100 stocks"),lty=c(1,3),col=c("black","red"),cex=0.5)
#check for cointegration
#entire period
create.spread.opt(cumul.index.ret,cumul100,0.4,0.05)
#### check for all combinations
from=10; to=40; by=5; dayss=180; cor_thresh=0.9; pval_thresh=0.01
for(j in seq(from,to,by)){
cat("\nNumber of stocks=",j,"\n\n")
index.j=index.retN(j,w.days,ret)[[2]]
for(i in 1:floor(length(cumul.index.ret)/dayss)){
spread=create.spread.opt(cumul.index.ret[(dayss*(i-1)):(dayss*i)],index.j[(dayss*(i-1)):(dayss*i)],cor_thresh,pval_thresh)[[1]]
print(c(spread,i))
}
rm(spread,index.j)
}
### double check using Johansen Test
maxlag=12*(dayss/100)^0.25
for(j in seq(from,to,by)){
cat("\nNumber of stocks=",j,"\n\n")
index.j=index.retN(j,w.days,ret)[[2]]
for(i in 1:floor(length(cumul.index.ret)/dayss)){
cumul.ret.all=log(cumul.index.ret[(dayss*(i-1)):(dayss*i)])
cumul.ret.j=log(index.j[(dayss*(i-1)):(dayss*i)])
data=data.frame(cumul.ret.all,cumul.ret.j)
varest=VAR(data,p=1,type="const",lag.max=maxlag,ic="AIC")
lagLength=max(maxlag,varest$p)
res=ca.jo(data,type="trace",ecdet="const",K=lagLength,spec="transitory")
testStatistics=res@teststat
criticalValues=res@cval
if(testStatistics[length(testStatistics)] >= criticalValues[length(testStatistics)+4]){
print(c(res@V[1:(ncol(data)+1),which.max(res@lambda)],i))
}
}
rm(index.j,cumul.ret.all,cumul.ret.j,data,varest,lagLength,res,testStatistics,criticalValues)
}
####################### KEEP WEIGHTS SAME FOR ANY GIVEN YEAR #######################
years=rep(0,25)
years[1]=sum(days[1:12])
for(i in 2:24){
years[i]=sum(days[1:(i*12)])-sum(days[1:((i-1)*12)])
}
years[25]=sum(days[289:291])
# January weights replicated over entire year
w.jan=matrix(nrow=25,ncol=dim(w)[2])
for(i in 1:25){
w.jan[i,]=w[1+(i-1)*12,]
}
w.jan.days=w.jan[rep(1:nrow(w.jan), times = years), ]
#cumulative returns
index.ret2=vector("numeric"); cumul.index.ret2=vector("numeric")
cumul.index.ret2[1]=100
for(i in 1:nrow(ret)){
index.ret2[i]=sum(ret[i,]*w.jan.days[i,])
cumul.index.ret2[i+1]=cumul.index.ret2[i]*(1+index.ret2[i])
}
#100 stock in chosen index
index2.100=index.retN(100,w.jan.days,ret)
ret2.100=index2.100[[1]]
cumul2.100=index2.100[[2]]
#diagnostic check
sd(ret2.100-index.ret2)
par(mfrow=c(2,1))
ts.plot(index.ret2,main="S&P 500 1.1.1990-19.3.2014",ylab="Daily log-returns of index")
lines(ret2.100,col="red",lty=3)
legend("bottomright",c("All stocks","Largest 100 stocks"),lty=c(1,3),col=c("black","red"),cex=0.5)
ts.plot(cumul.index.ret2,main="S&P 500 1.1.1990-19.3.2014",ylab="Daily cumulative log-returns of index")
lines(cumul2.100,col="red",lty=3)
legend("bottomright",c("All stocks","Largest 100 stocks"),lty=c(1,3),col=c("black","red"),cex=0.5)
#check for cointegration
#entire period
create.spread.opt(cumul.index.ret,cumul100,0.4,0.05)
#### check for all combinations
from2=10; to2=40; by2=5; days2=180; cor_thresh2=0.9; pval_thresh2=0.01
for(j in seq(from2,to2,by2)){
cat("\nNumber of stocks=",j,"\n\n")
index.j2=index.retN(j,w.jan.days,ret)[[2]]
for(i in 1:floor(length(cumul.index.ret2)/days2)){
spread2=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.j2[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[1]]
print(c(spread2,i))
}
rm(spread2,index.j2,i,j)
}
### double check using Johansen Test
maxlag2=12*(days2/100)^0.25
for(j in seq(from2,to2,by2)){
cat("\nNumber of stocks=",j,"\n\n")
index.j2=index.retN(j,w.jan.days,ret)[[2]]
for(i in 1:floor(length(cumul.index.ret2)/days2)){
cumul.ret.all2=log(cumul.index.ret2[(days2*(i-1)):(days2*i)])
cumul.ret.j2=log(index.j2[(days2*(i-1)):(days2*i)])
data2=data.frame(cumul.ret.all2,cumul.ret.j2)
varest2=VAR(data2,p=1,type="const",lag.max=maxlag2,ic="AIC")
lagLength2=max(maxlag2,varest2$p)
res2=ca.jo(data2,type="trace",ecdet="const",K=lagLength2,spec="transitory")
testStatistics2=res2@teststat
criticalValues2=res2@cval
if(testStatistics2[length(testStatistics2)] >= criticalValues2[length(testStatistics2)+4]){
print(c(res2@V[1:(ncol(data2)+1),which.max(res2@lambda)],i))
}
}
rm(index.j2,cumul.ret.all2,cumul.ret.j2,data2,varest2,lagLength2,res2,testStatistics2,criticalValues2,i,j)
}
### optimal to use 20 stocks
index.20=index.retN(20,w.jan.days,ret)[[2]]
i=3
spreadin3=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=4
spreadout3=log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.290711*log(index.20[(days2*(i-1)):(days2*i)])+10.642420
ts.plot(spreadin3); lines(spreadout3,col="red")
i=9
spreadin9=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=10
spreadout9=log(index.20[(days2*(i-1)):(days2*i)])-0.2673692*log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.3771963
ts.plot(spreadin9); lines(spreadout9,col="red")
i=11
spreadin11=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=12
spreadout11=log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.160233*log(index.20[(days2*(i-1)):(days2*i)])+9.950650
ts.plot(spreadin11); lines(spreadout11,col="red")
i=17
spreadin17=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=18
spreadout17=log(index.20[(days2*(i-1)):(days2*i)])-0.2716707*log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.4214602
ts.plot(spreadin17); lines(spreadout17,col="red")
i=31
spreadin31=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=32
spreadout31=log(index.20[(days2*(i-1)):(days2*i)])-0.2855611*log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.3344949
ts.plot(spreadin31); lines(spreadout31,col="red")
i=45
spreadin45=create.spread.opt(cumul.index.ret2[(days2*(i-1)):(days2*i)],index.20[(days2*(i-1)):(days2*i)],cor_thresh2,pval_thresh2)[[2]]
i=46
spreadout45=log(index.20[(days2*(i-1)):(days2*i)])-0.2845362*log(cumul.index.ret2[(days2*(i-1)):(days2*i)])-3.3266583
ts.plot(spreadin45); lines(spreadout45,col="red")
rm(i)
par(mfrow=c(4,3))
ts.plot(spreadin3)
ts.plot(spreadin9)
ts.plot(spreadin11)
ts.plot(spreadout3,col="red")
ts.plot(spreadout9,col="red")
ts.plot(spreadout11,col="red")
ts.plot(spreadin17)
ts.plot(spreadin31)
ts.plot(spreadin45)
ts.plot(spreadout17,col="red")
ts.plot(spreadout31,col="red")
ts.plot(spreadout45,col="red")
par(mfrow=c(4,3))
ts.plot(spreadin3)
ts.plot(spreadin9)
ts.plot(spreadin11)
ts.plot(spreadout3,col="red")
ts.plot(spreadout9,col="red")
ts.plot(spreadout11,col="red")
ts.plot(spreadin17)
ts.plot(spreadin31)
ts.plot(spreadin45)
ts.plot(spreadout17,col="red")
ts.plot(spreadout31,col="red")
ts.plot(spreadout45,col="red")
ts.plot(spreadin3)
dev.off()
ts.plot(spreadin9)
nsteps=5000; N=1000
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=spread3out) # sigmas tuning 0.5, 0.7, 0.9
source("PMCMC_functions.R")
setwd("/Users/tillkischkat/Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R")
("PMCMC_functions.R")
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=spread3out) # sigmas tuning 0.5, 0.7, 0.9
source("PMCMC_functions.R")
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=spread3out) # sigmas tuning 0.5, 0.7, 0.9
ts.plot(spreadin3)
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=spreadout3) # sigmas tuning 0.5, 0.7, 0.9
nsteps=5000; N=1000; df=5
setwd("/Users/tillkischkat/Desktop/MSc Statistics/Trimester 3/Statistical Arbitrage/R:MATLAB Code/PMCMC R")
#install.packages("truncnorm")
#install.packages("pscl")
#install.packages("coda")
#install.packages("doParallel")
library(truncnorm)
library(pscl)
library(coda)
library(doParallel)
source("PMCMC_functions.R")
nsteps=5000; N=1000; df=5
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=spreadout3) # sigmas tuning 0.5, 0.7, 0.9
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
acf(test.tau,main=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
effectiveSize(mcmc(test.rho))/nsteps
effectiveSize(mcmc(test.tau))/nsteps
effectiveSize(mcmc(test.sigma))/nsteps
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
effectiveSize(mcmc(test.sigma))/nsteps
summary(as.mcmc(test.sigma))
summary(as.mcmc(test.tau))
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
par(mfrow=c(4,3))
ts.plot(spreadin3)
ts.plot(spreadin9)
ts.plot(spreadin11)
ts.plot(spreadout3,col="red")
ts.plot(spreadout9,col="red")
ts.plot(spreadout11,col="red")
ts.plot(spreadin17)
ts.plot(spreadin31)
ts.plot(spreadin45)
ts.plot(spreadout17,col="red")
ts.plot(spreadout31,col="red")
ts.plot(spreadout45,col="red")
nsteps=2000; N=1000; df=5
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.1,sigma_sigma=0.1,nsteps=nsteps,y=10*spreadin3)
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
effectiveSize(mcmc(test.rho))/nsteps
effectiveSize(mcmc(test.tau))/nsteps
effectiveSize(mcmc(test.sigma))/nsteps
par(mfrow=c(2,2))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
nsteps=5000; N=1000; df=5; magnification=10
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.7,sigma_tau=0.7,sigma_sigma=0.7,nsteps=nsteps,y=magnification*spreadout3)
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
summary(as.mcmc(test.rho))
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.sigma))
nsteps=200; N=1000; df=5; magnification=10
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.7,sigma_tau=0.7,sigma_sigma=0.7,nsteps=nsteps,y=magnification*spreadout3)
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.2,sigma_tau=0.2,sigma_sigma=0.2,nsteps=nsteps,y=magnification*spreadout3)
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
### Diagnostics
summary(as.mcmc(test.rho))
summary(as.mcmc(test.tau))
summary(as.mcmc(test.sigma))
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
nsteps=25000; N=1000; df=5; magnification=10
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.15,sigma_sigma=0.15,nsteps=nsteps,y=magnification*spreadout3)
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
par(mfrow=c(2,2))
hist(test.rho,breaks=50,main="Histogram of rho with N=1000",xlab="Rho")
hist(test.tau,breaks=50,main="Histogram of tau with N=1000",xlab="Tau")
hist(test.sigma,breaks=50,main="Histogram of sigma with N=1000",xlab="Sigma")
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
### Diagnostics
summary(as.mcmc(test.rho))
summary(as.mcmc(test.tau))
summary(as.mcmc(test.sigma))
par(mfrow=c(3,2))
ts.plot(test.rho,main="Trace Plot",ylab=expression(rho))
acf(test.rho,main=expression(rho))
ts.plot(test.tau,main="Trace Plot",ylab=expression(tau))
acf(test.tau,main=expression(tau))
ts.plot(test.sigma,main="Trace Plot",ylab=expression(sigma))
acf(test.sigma,main=expression(sigma))
# effective sample size (proportion)
effectiveSize(mcmc(test.rho))/nsteps
effectiveSize(mcmc(test.tau))/nsteps
effectiveSize(mcmc(test.sigma))/nsteps
nsteps=200
df=2
test=gmhp.full(rho_0=0,tau_0=0.1,sigma_0=0.1,sigma_rho=0.1,sigma_tau=0.15,sigma_sigma=0.15,nsteps=nsteps,y=magnification*spreadout3)
test.rho=test[[1]][-(1:(floor(nsteps/10)))]
test.tau=test[[2]][-(1:(floor(nsteps/10)))]
test.sigma=test[[3]][-(1:(floor(nsteps/10)))]
loglik=log(test[[4]][1:(floor(nsteps/10))])
# acceptance probability
1-rejectionRate(mcmc(test.rho))
1-rejectionRate(mcmc(test.tau))
1-rejectionRate(mcmc(test.sigma))
plot(loglik,main="Burn-in phase",xlab="Iterations",ylab="log-likelihood",type="l")
